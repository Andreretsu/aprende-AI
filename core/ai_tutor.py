import os
import ollama  # Vamos usar o Ollama direto, sem passar pelo LangChain
from django.conf import settings

# Importa√ß√µes dos pacotes "sat√©lites" que costumam dar menos erro
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
except ImportError as e:
    print(f"‚ùå Erro de importa√ß√£o: {e}")
    print("Verifique se instalou: langchain-community langchain-chroma langchain-huggingface")

# Vari√°vel global para guardar o Banco de Dados (n√£o a Chain)
_db = None

def carregar_banco_vetorial():
    """
    Carrega o PDF e cria o √≠ndice de busca (Vector Store).
    """
    global _db
    
    if _db is not None:
        return _db

    print("üîÑ [IA] Carregando banco de dados vetorial...")
    
    pdf_path = os.path.join(settings.BASE_DIR, 'ApostilaPortugues.pdf')
    persist_directory = os.path.join(settings.BASE_DIR, 'db_chroma')
    
    # Configura√ß√£o de Embeddings (o tradutor texto -> n√∫meros)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # Se o banco j√° existe no disco, carrega ele (muito mais r√°pido)
    if os.path.exists(persist_directory) and os.listdir(persist_directory):
        print("   - Carregando do disco...")
        _db = Chroma(
            persist_directory=persist_directory, 
            embedding_function=embeddings
        )
    else:
        # Se n√£o existe, cria do zero
        if not os.path.exists(pdf_path):
            print(f"‚ùå [IA] Erro: Arquivo {pdf_path} n√£o encontrado.")
            return None

        print("   - Processando PDF (pode demorar um pouco)...")
        loader = PyPDFLoader(pdf_path)
        documentos = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        textos_divididos = text_splitter.split_documents(documentos)
        
        _db = Chroma.from_documents(
            documents=textos_divididos, 
            embedding=embeddings,
            persist_directory=persist_directory
        )
    
    print("‚úÖ [IA] Banco de dados pronto!")
    return _db

def perguntar_ao_tutor(pergunta):
    db = carregar_banco_vetorial()
    if not db:
        return "Erro: Material de estudo n√£o encontrado."
    
    try:
        # 1. Busca: Encontra os 3 par√°grafos mais parecidos com a pergunta no PDF
        docs = db.similarity_search(pergunta, k=3)
        
        # Junta o conte√∫do desses par√°grafos em um texto s√≥
        contexto = "\n\n".join([doc.page_content for doc in docs])
        
        # 2. Prompt: Monta a mensagem para o Ollama
        prompt_sistema = """Voc√™ √© um tutor paciente e did√°tico de alfabetiza√ß√£o. 
Use APENAS o contexto abaixo para responder √† pergunta do aluno. 
Se a resposta n√£o estiver no contexto, diga que n√£o sabe."""
        
        prompt_usuario = f"""
Contexto retirado da apostila:
{contexto}

Pergunta do aluno: 
{pergunta}
"""

        # 3. Gera√ß√£o: Chama o Ollama direto (sem LangChain no meio)
        print("ü§ñ Enviando para o Llama 3.2...")
        response = ollama.chat(model='llama3.2', messages=[
            {'role': 'system', 'content': prompt_sistema},
            {'role': 'user', 'content': prompt_usuario},
        ])
        
        return response['message']['content']

    except Exception as e:
        print(f"Erro na gera√ß√£o: {e}")
        return "Desculpe, tive um problema t√©cnico para responder."